# Corrective Feedback
type_name: CF
num_interact: 5000
reward_update: 100  # number of epochs every time updating the reward model TODO: try [10, 50, 200]
acc_target: 0.97
feed_type: 0  # 0: uniform_sampling
max_feedback: 10000
reward_schedule: 0  # 0: constant
check_label: False
seed_step: 1000  # steps to collect data for label query
query_batch_0: 32  # (if not early_advertising or demo_pretrain) the first time to train reward model when interaction step reaches seed_step
reward_save_interval: null  # 1 interval = 1 round that receive new feedback; if null, only save model at critical points (e.g. after pre-training, end of training)
r_hat_GT_coef: null  # If null, then only use r_hat, do not use GT; else r=GT+coef*r_hat
use_potential:
  ahead: False
  back: False
  agent_potential: True  # if ahead or back = True, then agent_potential=True means "using potential for action choice"; Otherwise
  discount: ${agent.algo.discount}
early_advertising:  # give feedback to all states in seed steps, as mentioned in a baseline in 《Action Advising with Advice Imitation in Deep Reinforcement Learning》
  flag: True
  early_update: 3000
  early_acc_target: 0.97
demo_pretrain:
  flag: False
oracle:
  exp_path: null  # directory to load trained agent as action oracle
  ckpt_id: null
  eps: 0.  # previous we use default eps_eval, which is 0.001
traj_based:  # if not traj_based, the it's segment_based
  # NOTE: if traj_based, then the data rendering to human are trajectories generated by current training agent (like trajectories generated when evaluate the agent)
  #       if segment_based, then the data rendering to human are segments samples from data buffer collected during agent training, which will include trajectories that have exploration actions
  flag: false
  max_num_traj: 100
  num_traj_0: 10  # number of trajs at itr==0
  reward_update_itr0: 2000  # update limit for itr==0
  num_traj: 10  # number of trajs at itr>0
  reward_update: 1000  # update limit for itr>0
  fb_ratio: 1.  # set this ratio to 1., means give CF feedback for all actions that are incorrect
  fb_per_traj: null
reward_model:
  _target_: new_utils.reward_models.corrective_rlpyt.CorrectiveRlpytRewardModel
  encoder_cfg: ${reward_model_cfg}  # TODO: should the model architecture for Q\pi\reward_model be different?
  B: ${sampler_b}
  ensemble_size: 3  # number of reward models
  use_best_one: False  # If True, use the one that output 
  reward_lr: 3e-4
  query_batch: 128  # batch size for asking queries that need oracle/human to judge
  is_traj_based: ${reward_cfg.traj_based.flag}
  query_traj: ${reward_cfg.traj_based.num_traj}
  train_batch_size: 128  # batch size for training reward model TODO: 128 is too slow, try larger values
  size_segment: 25  # trajectory segments to be compared. NOTE: in PEBBLE they use 50, but in RLHF for Atari they use 25
  cf_per_seg: 1  # corrective feedback per segment
  neighbor_size: 1  # choices: [1, 3, 5,...]. e.g. 1=only corrective timestep; 3: [t-1, t, t+1] all use corrective action at t as corrective actoins
  loss_margine: 1  # margine used in reward loss # TODO: tun this value based on activation function for reward predictor
  max_size: 100000  # maximal number of timesteps in reward model's data buffer
  activation: null  # for the outputs from reward models, choices in ['tanh', 'sig', 'relu', 'null']
  loss_square: True  # as explained in DQfD+RLHF, "An extra loss proportional to the square of the predicted rewards is added to impose a zero-mean Gaussian prior on the reward distribution"
  loss_square_coef: 1.  # if loss_square=True, this is the coefficient for the extra squared loss
  label_capacity: 10000  # capacity for labeled query buffer (PEBBLE didn't tune this value, default 5e5; RLHF2017 use 3000)
  # large_batch: 10  # for some sampling methods, we need larger batch size to select queries
  device: ${device}
  init_type: kaiming_lrelu
  mask_img_score: True
  env_name: ${env.env_name}
  loss_name: Exp  # ['MargineLoss', 'MargineLossMin0', 'MargineLossMin0Fix', 'Exp']
  clip_grad_norm: 50.
  exp_loss_beta: 1.
  gaussian_noise:
    flag: false
    amplitude: 0.1
  oracle_type: oq  # oe-oracle_entropy, oq-oracle_q_value
  softmax_tau: 1.0  # For calculating action distribution based on oracle's Q
  ckpt_path: null  # if not null, ckpt will be loaded from ckpts in this path
  reset:
    flag: False
    type: rnd  # type: [rnd, init],  rnd=re-randomization, init=re-initialization
    mlp_layers: 0  # reset the last several layers
    cnn_layers: 0
    init_ckpt_path: null
  normalize_reward_cfg:
    train: null  # [null, m2], m2=minmax
    eval: null  # [null, m2], m2=minmax
