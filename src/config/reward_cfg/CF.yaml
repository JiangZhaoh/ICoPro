# Corrective Feedback
type_name: CF
num_interact: 5000
reward_update: 10  # number of epochs every time updating the reward model TODO: try [10, 50, 200]
feed_type: 0  # 0: uniform_sampling
max_feedback: 10000
reward_schedule: 0
oracle:
  exp_path: null  # directory to load trained agent as action oracle
  ckpt_id: null
reward_model:
  _target_: new_utils.reward_models.corrective.CorrectiveRewardModel
  encoder_cfg: ${atari_model_cfg}  # TODO: should the model architecture for Q\pi\reward_model be different?
  # TODO: for Pong, test episode_end_penalty=0; test different episode_end_penalty value
  episode_end_penalty: -2  # According to RLHF: When providing synthetic oracle feedback we replace episode ends with a penalty in all games except Pong
  ensemble_size: 3  # number of reward models
  reward_lr: 3e-4
  query_batch: 128  # batch size for asking queries that need oracle/human to judge
  train_batch_size: 128  # batch size for training reward model TODO: 128 is too slow, try larger values
  size_segment: 25  # trajectory segments to be compared. NOTE: in PEBBLE they use 50, but in RLHF for Atari they use 25
  cf_per_seg: 2  # corrective feedback per segment
  neighbor_size: 1  # choices: [1, 3, 5,...]. e.g. 1=only corrective timestep; 3: [t-1, t, t+1] all use corrective action at t as corrective actoins
  loss_margine: 1  # margine used in reward loss
  max_size: 100000  # maximal number of timesteps in reward model's data buffer
  activation: tanh  # for the outputs from reward models, choices in ['tanh', 'sig', 'relu']
  label_capacity: 3000  # capacity for labeled query buffer (PEBBLE didn't tune this value, default 5e5; RLHF2017 use 3000)
  # large_batch: 10  # for some sampling methods, we need larger batch size to select queries
  device: ${device}
  init_type: ${agent.double_q_discrete_critic.init_type}
